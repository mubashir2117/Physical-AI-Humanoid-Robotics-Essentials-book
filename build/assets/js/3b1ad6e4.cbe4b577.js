"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[251],{963:(e,n,t)=>{t.d(n,{A:()=>o});t(6540);var a=t(4164);const s={callout:"callout_jYHE",info:"info_IOty",tip:"tip_s2nh",caution:"caution_w7Js",danger:"danger_zfsw",icon:"icon_Ghiv",content:"content_JMk4"};var i=t(4848);function o({type:e,children:n}){const t=function(e){switch(e){case"info":default:return"\u2139\ufe0f";case"tip":return"\ud83d\udca1";case"caution":return"\u26a0\ufe0f";case"danger":return"\u274c"}}(e),o=(0,a.A)("callout",s.callout,s[e]);return(0,i.jsxs)("div",{className:o,children:[(0,i.jsx)("div",{className:s.icon,children:t}),(0,i.jsx)("div",{className:s.content,children:n})]})}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var a=t(6540);const s={},i=a.createContext(s);function o(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(i.Provider,{value:n},e.children)}},8530:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>g,frontMatter:()=>r,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"module-4-vla/chapter-18-cognitive-planning","title":"Chapter 18 - Cognitive Planning","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-18-cognitive-planning.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-18-cognitive-planning","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-18-cognitive-planning","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"Muhammad Uzair","lastUpdatedAt":1765045729000,"sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 18 - Cognitive Planning"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 17 - Voice to Action","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-17-voice-to-action"},"next":{"title":"Chapter 19 - Multi-Modal Interaction","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-19-multi-modal-interaction"}}');var s=t(4848),i=t(8453),o=t(963);const r={sidebar_position:3,title:"Chapter 18 - Cognitive Planning"},l="Chapter 18: Cognitive Planning for Robotics",c={},p=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content with Code Examples",id:"content-with-code-examples",level:2},{value:"Mermaid Diagrams",id:"mermaid-diagrams",level:2},{value:"Callouts",id:"callouts",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-18-cognitive-planning-for-robotics",children:"Chapter 18: Cognitive Planning for Robotics"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand cognitive planning approaches in robotics"}),"\n",(0,s.jsx)(n.li,{children:"Implement planning systems that integrate knowledge and perception"}),"\n",(0,s.jsx)(n.li,{children:"Design planning systems that can adapt to changing environments"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"content-with-code-examples",children:"Content with Code Examples"}),"\n",(0,s.jsx)(n.p,{children:"Cognitive planning in robotics involves creating systems that can reason about complex tasks, integrate knowledge from various sources, and adapt their plans based on changing conditions."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nimport json\n\nclass TaskType(Enum):\n    NAVIGATE = "navigate"\n    MANIPULATE = "manipulate"\n    PERCEIVE = "perceive"\n    COMMUNICATE = "communicate"\n\nclass ObjectState(Enum):\n    UNKNOWN = "unknown"\n    PRESENT = "present"\n    ABSENT = "absent"\n    GRASPED = "grasped"\n\n@dataclass\nclass RobotState:\n    position: tuple  # (x, y, theta)\n    battery_level: float\n    objects: Dict[str, ObjectState]\n    last_perception_time: float\n\n@dataclass\nclass Task:\n    task_type: TaskType\n    target: str\n    parameters: Dict = None\n    priority: int = 1\n\nclass CognitivePlanner(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planner\')\n        \n        # Initialize robot state\n        self.robot_state = RobotState(\n            position=(0, 0, 0),\n            battery_level=1.0,\n            objects={},\n            last_perception_time=0\n        )\n        \n        # Initialize task queue\n        self.task_queue: List[Task] = []\n        \n        # Publishers\n        self.nav_goal_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\n        self.task_status_pub = self.create_publisher(String, \'/task_status\', 10)\n        \n        # Subscribers\n        self.perception_sub = self.create_subscription(\n            String, \n            \'/perception_update\', \n            self.perception_callback, \n            10\n        )\n        \n        self.command_sub = self.create_subscription(\n            String, \n            \'/high_level_command\', \n            self.command_callback, \n            10\n        )\n        \n        # Timer for planning cycle\n        self.planner_timer = self.create_timer(1.0, self.planning_cycle)\n\n    def perception_callback(self, msg: String):\n        """Update robot state based on perception"""\n        try:\n            perception_data = json.loads(msg.data)\n            self.robot_state.last_perception_time = self.get_clock().now().nanoseconds / 1e9\n            \n            # Update object states\n            if \'objects\' in perception_data:\n                for obj_name, obj_state in perception_data[\'objects\'].items():\n                    self.robot_state.objects[obj_name] = ObjectState(obj_state)\n                    \n            # Update position if available\n            if \'position\' in perception_data:\n                self.robot_state.position = tuple(perception_data[\'position\'])\n                \n        except Exception as e:\n            self.get_logger().error(f\'Error processing perception update: {e}\')\n\n    def command_callback(self, msg: String):\n        """Add high-level command to task queue"""\n        try:\n            command_data = json.loads(msg.data)\n            task_type_str = command_data.get(\'task_type\', \'navigate\')\n            \n            # Convert string to enum\n            try:\n                task_type = TaskType(task_type_str.upper())\n            except ValueError:\n                self.get_logger().error(f\'Invalid task type: {task_type_str}\')\n                return\n            \n            task = Task(\n                task_type=task_type,\n                target=command_data.get(\'target\', \'\'),\n                parameters=command_data.get(\'parameters\', {}),\n                priority=command_data.get(\'priority\', 1)\n            )\n            \n            # Insert task based on priority\n            inserted = False\n            for i, t in enumerate(self.task_queue):\n                if task.priority > t.priority:\n                    self.task_queue.insert(i, task)\n                    inserted = True\n                    break\n            \n            if not inserted:\n                self.task_queue.append(task)\n                \n            self.get_logger().info(f\'Added task: {task.task_type.value} to {task.target}\')\n            \n        except Exception as e:\n            self.get_logger().error(f\'Error processing command: {e}\')\n\n    def planning_cycle(self):\n        """Main cognitive planning cycle"""\n        if not self.task_queue:\n            return\n            \n        # Get the highest priority task\n        current_task = self.task_queue[0]\n        \n        # Check preconditions and update plan if needed\n        if self.check_task_preconditions(current_task):\n            # Execute task\n            self.execute_task(current_task)\n            \n            # Remove completed task\n            self.task_queue.pop(0)\n            \n            # Publish task completion\n            status_msg = String()\n            status_msg.data = f\'Task completed: {current_task.task_type.value} to {current_task.target}\'\n            self.task_status_pub.publish(status_msg)\n        else:\n            # Replan or wait\n            status_msg = String()\n            status_msg.data = f\'Waiting to satisfy preconditions for: {current_task.task_type.value}\'\n            self.task_status_pub.publish(status_msg)\n\n    def check_task_preconditions(self, task: Task) -> bool:\n        """Check if preconditions for task are satisfied"""\n        if task.task_type == TaskType.NAVIGATE:\n            # Navigation typically always possible (unless battery too low)\n            return self.robot_state.battery_level > 0.1\n        elif task.task_type == TaskType.MANIPULATE:\n            # Manipulation requires the object to be present\n            required_object = task.target\n            return (\n                self.robot_state.objects.get(required_object, ObjectState.UNKNOWN) == ObjectState.PRESENT\n            )\n        elif task.task_type == TaskType.PERCEIVE:\n            # Perception is always possible\n            return True\n        else:\n            return True\n\n    def execute_task(self, task: Task):\n        """Execute the given task"""\n        if task.task_type == TaskType.NAVIGATE:\n            self.execute_navigation_task(task)\n        elif task.task_type == TaskType.MANIPULATE:\n            self.execute_manipulation_task(task)\n        elif task.task_type == TaskType.PERCEIVE:\n            self.execute_perception_task(task)\n        elif task.task_type == TaskType.COMMUNICATE:\n            self.execute_communication_task(task)\n\n    def execute_navigation_task(self, task: Task):\n        """Execute navigation task"""\n        # In a real implementation, this would parse the target and publish a navigation goal\n        self.get_logger().info(f\'Navigating to: {task.target}\')\n        \n        # For this example, we\'ll just publish a dummy goal\n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.header.frame_id = \'map\'\n        goal_msg.pose.position.x = 1.0  # Example coordinates\n        goal_msg.pose.position.y = 1.0\n        goal_msg.pose.position.z = 0.0\n        goal_msg.pose.orientation.w = 1.0\n        \n        self.nav_goal_pub.publish(goal_msg)\n\n    def execute_manipulation_task(self, task: Task):\n        """Execute manipulation task"""\n        self.get_logger().info(f\'Manipulating object: {task.target}\')\n        # Implementation would depend on the specific manipulation system\n\n    def execute_perception_task(self, task: Task):\n        """Execute perception task"""\n        self.get_logger().info(f\'Performing perception task: {task.target}\')\n        # Implementation would trigger appropriate perception routines\n\n    def execute_communication_task(self, task: Task):\n        """Execute communication task"""\n        self.get_logger().info(f\'Communicating: {task.target}\')\n        # Implementation would trigger speech or other communication\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = CognitivePlanner()\n    \n    try:\n        rclpy.spin(planner)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"mermaid-diagrams",children:"Mermaid Diagrams"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\n    A[High-Level Goal] --\x3e B[Cognitive Planner]\n    B --\x3e C[Task Decomposition]\n    C --\x3e D[State Assessment]\n    D --\x3e E[Plan Generation]\n    E --\x3e F[Plan Execution]\n    F --\x3e G[Monitor & Adapt]\n    G --\x3e H{Plan Successful?}\n    H --\x3e|No| I[Plan Reconsideration]\n    H --\x3e|Yes| J[Next Task]\n    I --\x3e B\n    J --\x3e B\n    K[Sensor Data] --\x3e B\n    L[Knowledge Base] --\x3e B\n    M[Robot Capabilities] --\x3e B\n"})}),"\n",(0,s.jsx)(n.h2,{id:"callouts",children:"Callouts"}),"\n",(0,s.jsx)(o.A,{type:"info",children:(0,s.jsx)(n.p,{children:"Cognitive planning systems maintain a model of the world state and reason about how to achieve goals based on that model."})}),"\n",(0,s.jsx)(o.A,{type:"tip",children:(0,s.jsx)(n.p,{children:"Implement plan monitoring to detect when execution is not proceeding as expected, triggering replanning when necessary."})}),"\n",(0,s.jsx)(o.A,{type:"caution",children:(0,s.jsx)(n.p,{children:"Cognitive planning can be computationally intensive. Consider hierarchical approaches to make planning tractable for complex tasks."})}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement a cognitive planner for a simple pick-and-place task"}),"\n",(0,s.jsx)(n.li,{children:"Add plan monitoring and replanning capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Integrate knowledge about object affordances into the planning system"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cognitive planning considers world state and task requirements for decision making"}),"\n",(0,s.jsx)(n.li,{children:"Planning systems must handle uncertainty and adapt to changing conditions"}),"\n",(0,s.jsx)(n.li,{children:"Hierarchical planning helps manage complexity in cognitive systems"}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);
"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[801],{963:(e,n,t)=>{t.d(n,{A:()=>i});t(6540);var o=t(4164);const a={callout:"callout_jYHE",info:"info_IOty",tip:"tip_s2nh",caution:"caution_w7Js",danger:"danger_zfsw",icon:"icon_Ghiv",content:"content_JMk4"};var s=t(4848);function i({type:e,children:n}){const t=function(e){switch(e){case"info":default:return"\u2139\ufe0f";case"tip":return"\ud83d\udca1";case"caution":return"\u26a0\ufe0f";case"danger":return"\u274c"}}(e),i=(0,o.A)("callout",a.callout,a[e]);return(0,s.jsxs)("div",{className:i,children:[(0,s.jsx)("div",{className:a.icon,children:t}),(0,s.jsx)("div",{className:a.content,children:n})]})}},4365:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-4-vla/chapter-20-capstone-project","title":"Chapter 20 - Capstone Project","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-20-capstone-project.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-20-capstone-project","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-20-capstone-project","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"Muhammad Uzair","lastUpdatedAt":1765045729000,"sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Chapter 20 - Capstone Project"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 19 - Multi-Modal Interaction","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-19-multi-modal-interaction"}}');var a=t(4848),s=t(8453),i=t(963);const r={sidebar_position:5,title:"Chapter 20 - Capstone Project"},l="Chapter 20: Capstone Project - Integrating Physical AI Systems",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content with Code Examples",id:"content-with-code-examples",level:2},{value:"Mermaid Diagrams",id:"mermaid-diagrams",level:2},{value:"Callouts",id:"callouts",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Conclusion",id:"conclusion",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-20-capstone-project---integrating-physical-ai-systems",children:"Chapter 20: Capstone Project - Integrating Physical AI Systems"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Design and implement a complete Physical AI system integrating multiple technologies"}),"\n",(0,a.jsx)(n.li,{children:"Apply all concepts learned throughout the book in a cohesive project"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate and optimize the performance of integrated systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"content-with-code-examples",children:"Content with Code Examples"}),"\n",(0,a.jsx)(n.p,{children:"The capstone project brings together all the concepts learned in this book to create a comprehensive Physical AI system. We'll design and implement a robot that can understand natural language commands, navigate environments, perceive objects, and execute complex tasks."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import Image, LaserScan\nfrom cv_bridge import CvBridge\nimport json\nimport numpy as np\nimport time\nfrom typing import Dict, List, Optional\n\nclass CapstoneRobotController(Node):\n    def __init__(self):\n        super().__init__(\'capstone_robot_controller\')\n        \n        # Initialize components\n        self.bridge = CvBridge()\n        self.current_state = "idle"  # idle, navigating, manipulating, etc.\n        self.task_queue = []\n        self.robot_capabilities = {\n            \'navigation\': True,\n            \'manipulation\': False,  # Assuming a non-manipulator robot for this example\n            \'perception\': True,\n            \'communication\': True\n        }\n        \n        # Robot state\n        self.position = (0.0, 0.0, 0.0)  # x, y, theta\n        self.battery_level = 1.0\n        self.objects_detected = {}\n        self.goal_position = None\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\n        self.speech_pub = self.create_publisher(String, \'/tts_input\', 10)\n        self.status_pub = self.create_publisher(String, \'/robot_status\', 10)\n        \n        # Subscribers\n        self.voice_cmd_sub = self.create_subscription(\n            String,\n            \'/voice_command\',\n            self.voice_command_callback,\n            10\n        )\n        \n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n        \n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.lidar_callback,\n            10\n        )\n        \n        # Timer for state machine\n        self.state_timer = self.create_timer(0.1, self.state_machine)\n        \n        # Navigation feedback\n        self.nav_complete_sub = self.create_subscription(\n            Bool,\n            \'/navigation_complete\',\n            self.navigation_complete_callback,\n            10\n        )\n        \n        self.get_logger().info("Capstone Robot Controller initialized")\n        self.say("Capstone robot system initialized. Ready for commands.")\n\n    def voice_command_callback(self, msg: String):\n        """Process high-level voice commands"""\n        command = msg.data.lower()\n        self.get_logger().info(f"Received command: {command}")\n        \n        # Simple command parsing (would be more sophisticated in practice)\n        if "go to" in command or "navigate to" in command:\n            # Extract target location\n            if "kitchen" in command:\n                self.add_navigation_task((3.0, 2.0, 0.0), "kitchen")\n            elif "living room" in command:\n                self.add_navigation_task((1.0, -1.0, 1.57), "living room")\n            elif "bedroom" in command:\n                self.add_navigation_task((-2.0, 1.0, 3.14), "bedroom")\n            else:\n                self.say("I don\'t know where that location is. Please specify kitchen, living room, or bedroom.")\n                \n        elif "find" in command or "look for" in command:\n            # Find objects\n            if "ball" in command or "object" in command:\n                self.add_perception_task("ball")\n            elif "person" in command:\n                self.add_perception_task("person")\n                \n        elif "stop" in command:\n            self.stop_robot()\n            self.say("Robot stopped")\n            \n        elif "status" in command or "report" in command:\n            self.report_status()\n            \n        else:\n            self.say(f"I don\'t understand the command: {command}")\n\n    def add_navigation_task(self, target_pos: tuple, location_name: str):\n        """Add a navigation task to the queue"""\n        task = {\n            \'type\': \'navigation\',\n            \'target\': target_pos,\n            \'location_name\': location_name,\n            \'priority\': 1\n        }\n        \n        self.task_queue.append(task)\n        self.say(f"Navigation task added: going to {location_name}")\n\n    def add_perception_task(self, target_object: str):\n        """Add a perception task to the queue"""\n        task = {\n            \'type\': \'perception\',\n            \'target\': target_object,\n            \'priority\': 2  # Higher priority than navigation\n        }\n        \n        self.task_queue.insert(0, task)  # Insert at beginning for higher priority\n        self.say(f"Perception task added: looking for {target_object}")\n\n    def image_callback(self, msg: Image):\n        """Process image data for object detection"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            \n            # Detect objects in the image\n            detected_objects = self.detect_objects_in_image(cv_image)\n            \n            # Update robot\'s knowledge\n            for obj in detected_objects:\n                self.objects_detected[obj[\'name\']] = {\n                    \'position\': obj[\'position\'],\n                    \'confidence\': obj[\'confidence\'],\n                    \'timestamp\': time.time()\n                }\n                \n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {e}")\n\n    def lidar_callback(self, msg: LaserScan):\n        """Process LIDAR data for navigation and obstacle avoidance"""\n        # Update robot position based on odometry would happen here\n        # For this example, we just store the LIDAR data\n        \n        # Check for obstacles in the immediate vicinity\n        min_distance = min([r for r in msg.ranges if r != float(\'inf\')], default=float(\'inf\'))\n        \n        if min_distance < 0.5:  # Less than 0.5m is too close\n            # Emergency stop if obstacle too close\n            self.stop_robot()\n            self.say("Obstacle detected. Stopping robot.")\n\n    def navigation_complete_callback(self, msg: Bool):\n        """Handle navigation completion"""\n        if msg.data and self.current_state == "navigating":\n            self.current_state = "idle"\n            self.say("Navigation task completed.")\n\n    def detect_objects_in_image(self, image):\n        """Detect objects in the image using simple color-based detection"""\n        # Convert to HSV for color-based segmentation\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        \n        # Define color ranges for common objects\n        color_ranges = {\n            \'red_ball\': ([0, 50, 50], [10, 255, 255]),\n            \'blue_object\': ([100, 50, 50], [130, 255, 255]),\n            \'green_object\': ([40, 50, 50], [80, 255, 255])\n        }\n        \n        detected_objects = []\n        \n        for obj_name, (lower, upper) in color_ranges.items():\n            # Create mask for color range\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n            \n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            # Filter contours by area\n            for cnt in contours:\n                area = cv2.contourArea(cnt)\n                if area > 1000:  # Only consider objects larger than 1000 pixels\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(cnt)\n                    \n                    # Calculate approximate position relative to image center\n                    center_x = x + w/2\n                    center_y = y + h/2\n                    img_center_x = image.shape[1] / 2\n                    \n                    # Determine rough direction (left/right/center)\n                    if center_x < img_center_x - 50:\n                        direction = "left"\n                    elif center_x > img_center_x + 50:\n                        direction = "right"\n                    else:\n                        direction = "center"\n                        \n                    detected_objects.append({\n                        \'name\': obj_name,\n                        \'position\': direction,\n                        \'confidence\': min(area / 5000, 1.0)  # Normalize confidence\n                    })\n        \n        return detected_objects\n\n    def state_machine(self):\n        """Main state machine for robot behavior"""\n        if self.current_state == "idle" and self.task_queue:\n            # Execute next task\n            task = self.task_queue.pop(0)\n            \n            if task[\'type\'] == \'navigation\':\n                self.current_state = "navigating"\n                self.navigate_to_position(task[\'target\'], task[\'location_name\'])\n            elif task[\'type\'] == \'perception\':\n                self.current_state = "perceiving"\n                self.perceive_object(task[\'target\'])\n                \n        # Update status periodically\n        self.publish_status()\n\n    def navigate_to_position(self, target_pos: tuple, location_name: str):\n        """Navigate to a specific position"""\n        self.get_logger().info(f"Navigating to {location_name} at {target_pos}")\n        \n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.header.frame_id = \'map\'\n        goal_msg.pose.position.x = float(target_pos[0])\n        goal_msg.pose.position.y = float(target_pos[1])\n        goal_msg.pose.position.z = 0.0\n        \n        # Convert theta to quaternion\n        theta = target_pos[2]\n        goal_msg.pose.orientation.z = np.sin(theta / 2.0)\n        goal_msg.pose.orientation.w = np.cos(theta / 2.0)\n        \n        self.goal_pub.publish(goal_msg)\n        self.say(f"Navigating to {location_name}")\n\n    def perceive_object(self, target_object: str):\n        """Look for a specific object using camera and image processing"""\n        self.get_logger().info(f"Looking for {target_object}")\n        \n        # In this implementation, we\'ll check what was detected in the last image\n        if target_object in [obj.split(\'_\')[0] for obj in self.objects_detected.keys()]:\n            # Object found\n            obj = [k for k in self.objects_detected.keys() if target_object in k][0]\n            position = self.objects_detected[obj][\'position\']\n            confidence = self.objects_detected[obj][\'confidence\']\n            \n            self.say(f"Found {obj} in the {position} area with confidence {confidence:.2f}")\n        else:\n            # Object not found\n            self.say(f"Could not find {target_object} in the current view")\n            \n        self.current_state = "idle"  # Return to idle after perception task\n\n    def report_status(self):\n        """Report current robot status"""\n        status = {\n            \'state\': self.current_state,\n            \'position\': self.position,\n            \'battery\': self.battery_level,\n            \'detected_objects\': list(self.objects_detected.keys()),\n            \'tasks_pending\': len(self.task_queue)\n        }\n        \n        status_msg = String()\n        status_msg.data = json.dumps(status)\n        self.status_pub.publish(status_msg)\n        \n        self.say(f"Status: I am {self.current_state}, at position {self.position[:2]}, "\n                 f"battery at {self.battery_level*100:.1f}%, "\n                 f"detected {len(self.objects_detected)} objects, "\n                 f"{len(self.task_queue)} tasks pending")\n\n    def stop_robot(self):\n        """Stop the robot"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd)\n        self.current_state = "idle"\n\n    def say(self, text: str):\n        """Publish text for TTS"""\n        msg = String()\n        msg.data = text\n        self.speech_pub.publish(msg)\n        self.get_logger().info(f"Robot says: {text}")\n\n    def publish_status(self):\n        """Publish robot status periodically"""\n        status = {\n            \'state\': self.current_state,\n            \'timestamp\': time.time()\n        }\n        \n        status_msg = String()\n        status_msg.data = json.dumps(status)\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = CapstoneRobotController()\n    \n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"mermaid-diagrams",children:"Mermaid Diagrams"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\n    A[User Command] --\x3e B[Natural Language Processing]\n    B --\x3e C[Task Planning]\n    C --\x3e D[Multi-Modal Perception]\n    D --\x3e E[State Estimation]\n    E --\x3e F[Action Selection]\n    F --\x3e G[Navigation & Control]\n    G --\x3e H[Execution Monitoring]\n    H --\x3e I{Task Complete?}\n    I --\x3e|No| J[Replanning]\n    I --\x3e|Yes| K[Report Result]\n    J --\x3e C\n    K --\x3e L[Next Task]\n    L --\x3e C\n    M[Sensor Data] --\x3e D\n    N[Knowledge Base] --\x3e C\n    O[Robot State] --\x3e E\n    P[Environment Model] --\x3e G\n"})}),"\n",(0,a.jsx)(n.h2,{id:"callouts",children:"Callouts"}),"\n",(0,a.jsx)(i.A,{type:"info",children:(0,a.jsx)(n.p,{children:"The capstone project demonstrates how to integrate all the technologies covered in this book: navigation, perception, natural language processing, and multi-modal interaction."})}),"\n",(0,a.jsx)(i.A,{type:"tip",children:(0,a.jsx)(n.p,{children:"When implementing complex integrated systems, use state machines or behavior trees to manage the different operational modes and transitions."})}),"\n",(0,a.jsx)(i.A,{type:"caution",children:(0,a.jsx)(n.p,{children:"Integration is often the most challenging part of robotics projects. Plan for debugging time and implement good logging and visualization tools."})}),"\n",(0,a.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Extend the capstone project to include actual manipulation capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Integrate LLMs for high-level task planning in the capstone system"}),"\n",(0,a.jsx)(n.li,{children:"Implement a learning component that improves performance over time"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Integration of multiple technologies creates powerful Physical AI systems"}),"\n",(0,a.jsx)(n.li,{children:"State management and task planning are crucial for complex behaviors"}),"\n",(0,a.jsx)(n.li,{children:"Real-world deployment requires robustness to handle unexpected situations"}),"\n",(0,a.jsx)(n.li,{children:"Continuous evaluation and improvement are essential for practical systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"Congratulations on completing the Physical AI & Humanoid Robotics book! You've now learned:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"ROS 2 fundamentals and advanced concepts"}),"\n",(0,a.jsx)(n.li,{children:"Simulation environments and digital twins"}),"\n",(0,a.jsx)(n.li,{children:"Isaac platform for AI-powered robotics"}),"\n",(0,a.jsx)(n.li,{children:"Vision-language-action models and multimodal interaction"}),"\n",(0,a.jsx)(n.li,{children:"How to integrate all these technologies in practical systems"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You're now equipped to develop sophisticated Physical AI systems that bridge the gap between artificial intelligence and the physical world. The next step is to apply these concepts to your own robotics projects!"})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const a={},s=o.createContext(a);function i(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[478],{963:(e,n,a)=>{a.d(n,{A:()=>o});a(6540);var i=a(4164);const t={callout:"callout_jYHE",info:"info_IOty",tip:"tip_s2nh",caution:"caution_w7Js",danger:"danger_zfsw",icon:"icon_Ghiv",content:"content_JMk4"};var s=a(4848);function o({type:e,children:n}){const a=function(e){switch(e){case"info":default:return"\u2139\ufe0f";case"tip":return"\ud83d\udca1";case"caution":return"\u26a0\ufe0f";case"danger":return"\u274c"}}(e),o=(0,i.A)("callout",t.callout,t[e]);return(0,s.jsxs)("div",{className:o,children:[(0,s.jsx)("div",{className:t.icon,children:a}),(0,s.jsx)("div",{className:t.content,children:n})]})}},3816:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4-vla/chapter-19-multi-modal-interaction","title":"Chapter 19 - Multi-Modal Interaction","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-19-multi-modal-interaction.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-19-multi-modal-interaction","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-19-multi-modal-interaction","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"Muhammad Uzair","lastUpdatedAt":1765045729000,"sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 19 - Multi-Modal Interaction"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 18 - Cognitive Planning","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-18-cognitive-planning"},"next":{"title":"Chapter 20 - Capstone Project","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-20-capstone-project"}}');var t=a(4848),s=a(8453),o=a(963);const l={sidebar_position:4,title:"Chapter 19 - Multi-Modal Interaction"},r="Chapter 19: Multi-Modal Interaction in Robotics",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content with Code Examples",id:"content-with-code-examples",level:2},{value:"Mermaid Diagrams",id:"mermaid-diagrams",level:2},{value:"Callouts",id:"callouts",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-19-multi-modal-interaction-in-robotics",children:"Chapter 19: Multi-Modal Interaction in Robotics"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Design multi-modal interfaces that combine different sensing and communication modalities"}),"\n",(0,t.jsx)(n.li,{children:"Integrate visual, auditory, and haptic feedback for enhanced interaction"}),"\n",(0,t.jsx)(n.li,{children:"Implement cross-modal reasoning for robust human-robot interaction"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"content-with-code-examples",children:"Content with Code Examples"}),"\n",(0,t.jsx)(n.p,{children:"Multi-modal interaction in robotics combines multiple sensory channels (vision, audio, touch, etc.) to create more natural and robust human-robot interfaces."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CompressedImage, LaserScan\nfrom std_msgs.msg import String, Float32MultiArray\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nimport json\n\nclass MultiModalInteractionNode(Node):\n    def __init__(self):\n        super().__init__('multi_modal_interaction')\n        \n        # Initialize CvBridge for image processing\n        self.bridge = CvBridge()\n        \n        # Internal state for multi-modal fusion\n        self.vision_data = None\n        self.audio_input = \"\"\n        self.tactile_data = None\n        self.fusion_result = None\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.interaction_pub = self.create_publisher(String, '/interaction_result', 10)\n        self.fusion_pub = self.create_publisher(Float32MultiArray, '/fused_data', 10)\n        \n        # Subscribers for different modalities\n        self.image_sub = self.create_subscription(\n            Image, \n            '/camera/rgb/image_raw', \n            self.image_callback, \n            10\n        )\n        \n        self.compressed_image_sub = self.create_subscription(\n            CompressedImage, \n            '/camera/rgb/image_compressed', \n            self.compressed_image_callback, \n            10\n        )\n        \n        self.audio_sub = self.create_subscription(\n            String, \n            '/audio_transcription', \n            self.audio_callback, \n            10\n        )\n        \n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.lidar_callback,\n            10\n        )\n        \n        # Timer for multi-modal fusion\n        self.fusion_timer = self.create_timer(0.5, self.multi_modal_fusion)\n\n    def image_callback(self, msg: Image):\n        \"\"\"Process RGB image data\"\"\"\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            \n            # Perform basic object detection\n            objects = self.detect_objects(cv_image)\n            \n            # Store vision data\n            self.vision_data = {\n                'timestamp': self.get_clock().now().nanoseconds,\n                'objects': objects,\n                'image_shape': cv_image.shape\n            }\n            \n            self.get_logger().info(f'Detected {len(objects)} objects in image')\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def compressed_image_callback(self, msg: CompressedImage):\n        \"\"\"Process compressed image data (alternative to raw image)\"\"\"\n        try:\n            # Convert compressed image to OpenCV\n            np_arr = np.frombuffer(msg.data, np.uint8)\n            cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n            \n            # Extract visual features\n            features = self.extract_visual_features(cv_image)\n            \n            # Update vision data with features\n            if self.vision_data:\n                self.vision_data['features'] = features\n            else:\n                self.vision_data = {\n                    'timestamp': self.get_clock().now().nanoseconds,\n                    'features': features,\n                    'image_shape': cv_image.shape\n                }\n                \n        except Exception as e:\n            self.get_logger().error(f'Error processing compressed image: {e}')\n\n    def audio_callback(self, msg: String):\n        \"\"\"Process audio transcription\"\"\"\n        self.audio_input = msg.data\n        self.get_logger().info(f'Audio input: {self.audio_input}')\n\n    def lidar_callback(self, msg: LaserScan):\n        \"\"\"Process LIDAR data for spatial awareness\"\"\"\n        # Extract relevant information from LIDAR scan\n        ranges = np.array(msg.ranges)\n        \n        # Filter out invalid ranges\n        valid_ranges = ranges[(ranges > msg.range_min) & (ranges < msg.range_max)]\n        \n        # Calculate basic spatial features\n        if len(valid_ranges) > 0:\n            nearest_obstacle = np.min(valid_ranges)\n            free_space_ahead = np.mean(valid_ranges[len(valid_ranges)//2 - 10:len(valid_ranges)//2 + 10])\n        else:\n            nearest_obstacle = float('inf')\n            free_space_ahead = float('inf')\n        \n        # Store LIDAR data\n        self.lidar_data = {\n            'timestamp': self.get_clock().now().nanoseconds,\n            'nearest_obstacle': nearest_obstacle,\n            'free_space_ahead': free_space_ahead,\n            'valid_ranges_count': len(valid_ranges)\n        }\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in image using simple color-based segmentation\"\"\"\n        # Convert to HSV for color-based segmentation\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        \n        # Define color ranges for common objects\n        color_ranges = {\n            'red': ([0, 50, 50], [10, 255, 255]),\n            'blue': ([100, 50, 50], [130, 255, 255]),\n            'green': ([40, 50, 50], [80, 255, 255])\n        }\n        \n        objects = []\n        for color_name, (lower, upper) in color_ranges.items():\n            # Create mask for color range\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n            \n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            # Filter contours by area\n            for cnt in contours:\n                area = cv2.contourArea(cnt)\n                if area > 500:  # Only consider objects larger than 500 pixels\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(cnt)\n                    objects.append({\n                        'color': color_name,\n                        'bbox': [x, y, x+w, y+h],\n                        'area': area\n                    })\n        \n        return objects\n\n    def extract_visual_features(self, image):\n        \"\"\"Extract visual features from image\"\"\"\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        \n        # Compute histogram\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n        \n        # Compute edges\n        edges = cv2.Canny(gray, 50, 150)\n        \n        # Calculate basic statistics\n        features = {\n            'mean_intensity': float(np.mean(gray)),\n            'std_intensity': float(np.std(gray)),\n            'edge_density': float(np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])),\n            'histogram': hist.flatten().tolist()\n        }\n        \n        return features\n\n    def multi_modal_fusion(self):\n        \"\"\"Fuse data from multiple modalities\"\"\"\n        # Check if we have data from all modalities\n        all_modalities_available = True\n        modalities = []\n        \n        if self.vision_data:\n            modalities.append(('vision', self.vision_data))\n        else:\n            all_modalities_available = False\n            \n        if self.audio_input:\n            modalities.append(('audio', self.audio_input))\n        else:\n            all_modalities_available = False\n            \n        if hasattr(self, 'lidar_data') and self.lidar_data:\n            modalities.append(('lidar', self.lidar_data))\n        else:\n            all_modalities_available = False\n        \n        # Perform fusion if all modalities are available\n        if all_modalities_available:\n            self.get_logger().info('Performing multi-modal fusion')\n            \n            # Simple fusion: combine all modalities into a single representation\n            fusion_result = {\n                'timestamp': self.get_clock().now().nanoseconds,\n                'fused_modalities': [m[0] for m in modalities],\n                'spatial_context': self.lidar_data['free_space_ahead'],\n                'visual_content': [obj['color'] for obj in self.vision_data['objects']],\n                'audio_content': self.audio_input\n            }\n            \n            # Cross-modal reasoning: if audio mentions an object color, find it in vision\n            if self.audio_input:\n                for obj in self.vision_data['objects']:\n                    if obj['color'] in self.audio_input.lower():\n                        fusion_result['target_object'] = obj\n                        self.get_logger().info(f'Found target object based on audio: {obj}')\n                        \n            self.fusion_result = fusion_result\n            \n            # Publish fused data\n            fusion_msg = Float32MultiArray()\n            # Convert fusion result to array format for publishing\n            # (in a real implementation, this would be more sophisticated)\n            fusion_msg.data = [float(self.lidar_data['free_space_ahead']), \n                              len(self.vision_data['objects']), \n                              len(self.audio_input)]\n            self.fusion_pub.publish(fusion_msg)\n            \n            # Publish interaction result\n            result_msg = String()\n            result_msg.data = json.dumps(fusion_result)\n            self.interaction_pub.publish(result_msg)\n        else:\n            # Not all modalities available, but still try to make sense of available data\n            partial_fusion = {\n                'timestamp': self.get_clock().now().nanoseconds,\n                'available_modalities': [m[0] for m in modalities],\n                'is_complete': False\n            }\n            \n            self.fusion_result = partial_fusion\n            \n            result_msg = String()\n            result_msg.data = json.dumps(partial_fusion)\n            self.interaction_pub.publish(result_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    interaction_node = MultiModalInteractionNode()\n    \n    try:\n        rclpy.spin(interaction_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        interaction_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"mermaid-diagrams",children:"Mermaid Diagrams"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\n    A[Human User] --\x3e B[Visual Modality]\n    A --\x3e C[Audio Modality]\n    A --\x3e D[Tactile Modality]\n    A --\x3e E[Spatial Modality]\n    B --\x3e F[Multi-Modal Fusion]\n    C --\x3e F\n    D --\x3e F\n    E --\x3e F\n    F --\x3e G[Cross-Modal Reasoning]\n    G --\x3e H[Unified Representation]\n    H --\x3e I[Robot Action]\n    I --\x3e J[Robot Feedback]\n    J --\x3e A\n"})}),"\n",(0,t.jsx)(n.h2,{id:"callouts",children:"Callouts"}),"\n",(0,t.jsx)(o.A,{type:"info",children:(0,t.jsx)(n.p,{children:"Multi-modal systems can provide robustness by using alternative modalities when one fails or is ambiguous."})}),"\n",(0,t.jsx)(o.A,{type:"tip",children:(0,t.jsx)(n.p,{children:"Implement cross-modal attention mechanisms that allow information from one modality to guide processing in another."})}),"\n",(0,t.jsx)(o.A,{type:"caution",children:(0,t.jsx)(n.p,{children:"Fusing information across modalities requires careful temporal alignment and handling of different data rates."})}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement a multi-modal system that combines vision and speech for object manipulation"}),"\n",(0,t.jsx)(n.li,{children:"Create a cross-modal attention mechanism for a robot interface"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate the robustness of multi-modal vs. single-modal interaction"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multi-modal interaction combines different sensory channels for richer interaction"}),"\n",(0,t.jsx)(n.li,{children:"Cross-modal reasoning enables more robust and natural interaction"}),"\n",(0,t.jsx)(n.li,{children:"Temporal alignment is critical when fusing different modalities"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>l});var i=a(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);
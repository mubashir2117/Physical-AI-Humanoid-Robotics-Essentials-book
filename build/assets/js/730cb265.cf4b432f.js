"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[669],{963:(e,n,t)=>{t.d(n,{A:()=>i});t(6540);var a=t(4164);const o={callout:"callout_jYHE",info:"info_IOty",tip:"tip_s2nh",caution:"caution_w7Js",danger:"danger_zfsw",icon:"icon_Ghiv",content:"content_JMk4"};var s=t(4848);function i({type:e,children:n}){const t=function(e){switch(e){case"info":default:return"\u2139\ufe0f";case"tip":return"\ud83d\udca1";case"caution":return"\u26a0\ufe0f";case"danger":return"\u274c"}}(e),i=(0,a.A)("callout",o.callout,o[e]);return(0,s.jsxs)("div",{className:i,children:[(0,s.jsx)("div",{className:o.icon,children:t}),(0,s.jsx)("div",{className:o.content,children:n})]})}},2761:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-4-vla/chapter-16-llms-and-robotics","title":"Chapter 16 - LLMs and Robotics","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-16-llms-and-robotics.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-16-llms-and-robotics","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-16-llms-and-robotics","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"Muhammad Uzair","lastUpdatedAt":1765045729000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Chapter 16 - LLMs and Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 15 - Nav2","permalink":"/humanoid-robotics/docs/module-3-isaac/chapter-15-nav2"},"next":{"title":"Chapter 17 - Voice to Action","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-17-voice-to-action"}}');var o=t(4848),s=t(8453),i=t(963);const r={sidebar_position:1,title:"Chapter 16 - LLMs and Robotics"},l="Chapter 16: Large Language Models and Robotics",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content with Code Examples",id:"content-with-code-examples",level:2},{value:"Mermaid Diagrams",id:"mermaid-diagrams",level:2},{value:"Callouts",id:"callouts",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-16-large-language-models-and-robotics",children:"Chapter 16: Large Language Models and Robotics"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand how large language models (LLMs) can be integrated with robotics systems"}),"\n",(0,o.jsx)(n.li,{children:"Implement natural language interfaces for robot control"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate the benefits and challenges of LLM-robot integration"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"content-with-code-examples",children:"Content with Code Examples"}),"\n",(0,o.jsx)(n.p,{children:"Large Language Models (LLMs) like GPT, Claude, and specialized models are increasingly being integrated with robotics systems to enable natural language interaction, task planning, and high-level reasoning."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nimport rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nimport json\n\nclass LLMRobotController(Node):\n    def __init__(self):\n        super().__init__(\'llm_robot_controller\')\n        \n        # Set OpenAI API key (in a real application, this would be in a secure config)\n        openai.api_key = "YOUR_API_KEY"\n        \n        # Create publishers and subscribers\n        self.nav_goal_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\n        self.cmd_pub = self.create_publisher(String, \'/robot_command\', 10)\n        self.voice_sub = self.create_subscription(\n            String, \n            \'/voice_command\', \n            self.voice_command_callback, \n            10\n        )\n        \n        # Timer to process high-level commands\n        self.timer = self.create_timer(1.0, self.process_commands)\n\n        self.pending_commands = []\n\n    def voice_command_callback(self, msg: String):\n        """Handle incoming voice commands"""\n        command_text = msg.data\n        self.get_logger().info(f\'Received voice command: {command_text}\')\n        \n        # Process command through LLM to generate robot actions\n        robot_action = self.process_command_with_llm(command_text)\n        self.pending_commands.append(robot_action)\n\n    def process_command_with_llm(self, command: str) -> dict:\n        """Process natural language command using LLM"""\n        prompt = f"""\n        Given the following robot command, convert it into structured robot actions.\n        The robot capabilities are: navigation, object manipulation, grasping, speaking, and light control.\n        Respond in JSON format with action_type and parameters.\n        \n        Command: "{command}"\n        \n        Example response structure:\n        {{\n            "action_type": "navigation",\n            "parameters": {{\n                "destination": [x, y, theta]\n            }}\n        }}\n        \n        Response:\n        """\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.1\n            )\n            \n            # Parse the LLM response\n            action_str = response.choices[0].message[\'content\'].strip()\n            \n            # Extract JSON from response (in a real implementation, this would be more robust)\n            start_idx = action_str.find(\'{\')\n            end_idx = action_str.rfind(\'}\') + 1\n            json_str = action_str[start_idx:end_idx]\n            \n            return json.loads(json_str)\n        except Exception as e:\n            self.get_logger().error(f\'Error processing command with LLM: {e}\')\n            return {"action_type": "error", "parameters": {}}\n\n    def process_commands(self):\n        """Process pending commands"""\n        if not self.pending_commands:\n            return\n            \n        command = self.pending_commands.pop(0)\n        \n        if command["action_type"] == "navigation":\n            self.execute_navigation(command["parameters"])\n        elif command["action_type"] == "manipulation":\n            self.execute_manipulation(command["parameters"])\n        elif command["action_type"] == "speak":\n            self.execute_speak(command["parameters"])\n\n    def execute_navigation(self, params):\n        """Execute navigation command"""\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = \'map\'\n        \n        dest = params.get("destination", [0, 0, 0])\n        pose_msg.pose.position.x = float(dest[0])\n        pose_msg.pose.position.y = float(dest[1])\n        pose_msg.pose.position.z = 0.0\n        \n        # Convert theta to quaternion\n        theta = dest[2]\n        pose_msg.pose.orientation.z = math.sin(theta / 2.0)\n        pose_msg.pose.orientation.w = math.cos(theta / 2.0)\n        \n        self.nav_goal_pub.publish(pose_msg)\n        self.get_logger().info(f\'Navigating to: {dest}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = LLMRobotController()\n    \n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"mermaid-diagrams",children:"Mermaid Diagrams"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\n    A[User Natural Language] --\x3e B[Large Language Model]\n    B --\x3e C[Structured Robot Actions]\n    C --\x3e D[Navigation]\n    C --\x3e E[Manipulation]\n    C --\x3e F[Communication]\n    D --\x3e G[Robot Movement]\n    E --\x3e H[Robot Manipulator]\n    F --\x3e I[Speech/Display]\n    J[Robot Sensors] --\x3e B\n    K[Environment State] --\x3e B\n"})}),"\n",(0,o.jsx)(n.h2,{id:"callouts",children:"Callouts"}),"\n",(0,o.jsx)(i.A,{type:"info",children:(0,o.jsx)(n.p,{children:"LLMs can bridge the gap between high-level human instructions and low-level robot actions, enabling more intuitive human-robot interaction."})}),"\n",(0,o.jsx)(i.A,{type:"tip",children:(0,o.jsx)(n.p,{children:"When using LLMs with robots, implement safety checks to ensure that generated actions are appropriate and safe for the environment."})}),"\n",(0,o.jsx)(i.A,{type:"caution",children:(0,o.jsx)(n.p,{children:"LLMs can generate plausible but incorrect responses. Always validate and verify commands before execution on physical robots."})}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a natural language interface for a simulated robot"}),"\n",(0,o.jsx)(n.li,{children:"Create a safety validation system for LLM-generated commands"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate the effectiveness of LLM-based task planning vs. traditional methods"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"LLMs enable natural language interaction with robots"}),"\n",(0,o.jsx)(n.li,{children:"They can generate complex action sequences from high-level commands"}),"\n",(0,o.jsx)(n.li,{children:"Safety validation is critical when executing LLM-generated commands"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var a=t(6540);const o={},s=a.createContext(o);function i(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);
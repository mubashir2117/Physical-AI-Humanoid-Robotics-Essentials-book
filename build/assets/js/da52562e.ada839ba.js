"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[425],{963:(e,n,t)=>{t.d(n,{A:()=>r});t(6540);var o=t(4164);const i={callout:"callout_jYHE",info:"info_IOty",tip:"tip_s2nh",caution:"caution_w7Js",danger:"danger_zfsw",icon:"icon_Ghiv",content:"content_JMk4"};var s=t(4848);function r({type:e,children:n}){const t=function(e){switch(e){case"info":default:return"\u2139\ufe0f";case"tip":return"\ud83d\udca1";case"caution":return"\u26a0\ufe0f";case"danger":return"\u274c"}}(e),r=(0,o.A)("callout",i.callout,i[e]);return(0,s.jsxs)("div",{className:r,children:[(0,s.jsx)("div",{className:i.icon,children:t}),(0,s.jsx)("div",{className:i.content,children:n})]})}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var o=t(6540);const i={},s=o.createContext(i);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(s.Provider,{value:n},e.children)}},8807:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-4-vla/chapter-17-voice-to-action","title":"Chapter 17 - Voice to Action","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-17-voice-to-action.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-17-voice-to-action","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-17-voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"Muhammad Uzair","lastUpdatedAt":1765045729000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chapter 17 - Voice to Action"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 16 - LLMs and Robotics","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-16-llms-and-robotics"},"next":{"title":"Chapter 18 - Cognitive Planning","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-18-cognitive-planning"}}');var i=t(4848),s=t(8453),r=t(963);const a={sidebar_position:2,title:"Chapter 17 - Voice to Action"},c="Chapter 17: Voice to Action Systems",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content with Code Examples",id:"content-with-code-examples",level:2},{value:"Mermaid Diagrams",id:"mermaid-diagrams",level:2},{value:"Callouts",id:"callouts",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-17-voice-to-action-systems",children:"Chapter 17: Voice to Action Systems"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Build voice-to-action systems for robotics applications"}),"\n",(0,i.jsx)(n.li,{children:"Integrate speech recognition with robot control"}),"\n",(0,i.jsx)(n.li,{children:"Implement context-aware voice command interpretation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"content-with-code-examples",children:"Content with Code Examples"}),"\n",(0,i.jsx)(n.p,{children:"Voice to action systems enable robots to understand and execute spoken commands, providing a natural interface for human-robot interaction."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nimport time\n\nclass VoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_to_action\')\n        \n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        \n        # Set up ROS publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.speech_pub = self.create_publisher(String, \'/tts_input\', 10)\n        \n        # Subscribe to sensor data for context awareness\n        self.lidar_sub = self.create_subscription(\n            LaserScan, \n            \'/scan\', \n            self.lidar_callback, \n            10\n        )\n        \n        # Robot state\n        self.lidar_data = None\n        self.last_command_time = time.time()\n        \n        # Start voice recognition\n        self.get_logger().info(\'Starting voice recognition...\')\n        self.listen_for_commands()\n\n    def lidar_callback(self, msg: LaserScan):\n        """Update lidar data for context awareness"""\n        self.lidar_data = msg.ranges\n\n    def listen_for_commands(self):\n        """Continuously listen for voice commands"""\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n        \n        self.get_logger().info(\'Listening for commands...\')\n        \n        while rclpy.ok():\n            try:\n                with self.microphone as source:\n                    # Listen for audio with timeout\n                    audio = self.recognizer.listen(source, timeout=5.0, phrase_time_limit=5.0)\n                \n                # Recognize speech using Google Web Speech API\n                command_text = self.recognizer.recognize_google(audio).lower()\n                self.get_logger().info(f\'Heard command: {command_text}\')\n                \n                # Process the command\n                self.process_voice_command(command_text)\n                \n            except sr.WaitTimeoutError:\n                # No speech detected within timeout, continue listening\n                pass\n            except sr.UnknownValueError:\n                self.get_logger().info(\'Could not understand audio\')\n            except sr.RequestError as e:\n                self.get_logger().error(f\'Error with speech recognition service: {e}\')\n            except Exception as e:\n                self.get_logger().error(f\'Unexpected error: {e}\')\n\n    def process_voice_command(self, command: str):\n        """Process recognized voice command"""\n        # Context-aware processing\n        if \'stop\' in command:\n            self.stop_robot()\n        elif \'forward\' in command or \'go\' in command:\n            if self.is_path_clear():\n                self.move_forward()\n            else:\n                self.say("Path is blocked, cannot move forward")\n        elif \'backward\' in command:\n            self.move_backward()\n        elif \'turn left\' in command or \'left\' in command:\n            self.turn_left()\n        elif \'turn right\' in command or \'right\' in command:\n            self.turn_right()\n        elif \'spin\' in command:\n            self.spin()\n        elif \'help\' in command:\n            self.provide_help()\n        else:\n            self.say(f"I didn\'t understand the command: {command}")\n        \n        self.last_command_time = time.time()\n\n    def is_path_clear(self) -> bool:\n        """Check if path ahead is clear using lidar data"""\n        if self.lidar_data is None:\n            return True  # If no data available, assume path is clear\n        \n        # Check the front 30-degree sector\n        front_ranges = self.lidar_data[len(self.lidar_data)//2 - 15 : len(self.lidar_data)//2 + 15]\n        min_distance = min([r for r in front_ranges if r != float(\'inf\')], default=float(\'inf\'))\n        \n        # Path is clear if no obstacles within 1 meter\n        return min_distance > 1.0\n\n    def stop_robot(self):\n        """Stop the robot"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd)\n        self.say("Stopping robot")\n\n    def move_forward(self):\n        """Move robot forward"""\n        cmd = Twist()\n        cmd.linear.x = 0.5  # m/s\n        cmd.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd)\n        self.say("Moving forward")\n\n    def move_backward(self):\n        """Move robot backward"""\n        cmd = Twist()\n        cmd.linear.x = -0.5  # m/s\n        cmd.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd)\n        self.say("Moving backward")\n\n    def turn_left(self):\n        """Turn robot left"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.5  # rad/s\n        self.cmd_vel_pub.publish(cmd)\n        self.say("Turning left")\n\n    def turn_right(self):\n        """Turn robot right"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = -0.5  # rad/s\n        self.cmd_vel_pub.publish(cmd)\n        self.say("Turning right")\n\n    def spin(self):\n        """Spin robot in place"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 1.0  # rad/s\n        self.cmd_vel_pub.publish(cmd)\n        self.say("Spinning")\n\n    def say(self, text: str):\n        """Publish text for TTS system"""\n        msg = String()\n        msg.data = text\n        self.speech_pub.publish(msg)\n        self.get_logger().info(f"Robot says: {text}")\n\n    def provide_help(self):\n        """Provide help on available commands"""\n        help_text = "Available commands: move forward, turn left, turn right, spin, stop, help"\n        self.say(help_text)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    \n    # Set up speech recognition parameters\n    recognizer = sr.Recognizer()\n    recognizer.energy_threshold = 4000  # Adjust for ambient noise\n    \n    try:\n        voice_node = VoiceToActionNode()\n        rclpy.spin(voice_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"mermaid-diagrams",children:"Mermaid Diagrams"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\n    A[Human Speech] --\x3e B[Speech Recognition]\n    B --\x3e C[Natural Language Understanding]\n    C --\x3e D[Context Integration]\n    D --\x3e E[Action Planning]\n    E --\x3e F[Robot Action Execution]\n    G[Robot Sensors] --\x3e D\n    H[Environment Context] --\x3e D\n    I[Robot State] --\x3e D\n    J[Speech Output] --\x3e A\n"})}),"\n",(0,i.jsx)(n.h2,{id:"callouts",children:"Callouts"}),"\n",(0,i.jsx)(r.A,{type:"info",children:(0,i.jsx)(n.p,{children:"Context-aware voice systems consider sensor data and robot state when interpreting commands, making them more robust and useful."})}),"\n",(0,i.jsx)(r.A,{type:"tip",children:(0,i.jsx)(n.p,{children:"Implement timeouts and error handling in voice recognition systems to prevent the robot from getting stuck waiting for commands."})}),"\n",(0,i.jsx)(r.A,{type:"caution",children:(0,i.jsx)(n.p,{children:"Voice recognition accuracy can vary with ambient noise, accents, and other factors. Always provide feedback to users about recognized commands."})}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Build a voice-controlled robot in simulation with safety constraints"}),"\n",(0,i.jsx)(n.li,{children:"Implement context-awareness based on sensor data for voice commands"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate voice recognition performance in different acoustic environments"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Voice-to-action systems provide intuitive human-robot interaction"}),"\n",(0,i.jsx)(n.li,{children:"Context awareness improves reliability of voice commands"}),"\n",(0,i.jsx)(n.li,{children:"Proper error handling is essential for usable systems"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);